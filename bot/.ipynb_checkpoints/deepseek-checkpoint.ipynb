{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964b7a9-da1f-468f-a300-907b54727b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate torch fastapi uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba431e9-607b-4e42-ba98-ed10e6db8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# Save locally\n",
    "tokenizer.save_pretrained(\"./DeepSeek-R1\")\n",
    "model.save_pretrained(\"./DeepSeek-R1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40700574-5859-4ce3-a965-be44b77ba2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "\n",
    "# # Load from local directory\n",
    "# MODEL_PATH = \"./DeepSeek-R1\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_PATH,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model.eval()\n",
    "\n",
    "# def generate_response(prompt: str, max_tokens: int = 512) -> str:\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c737bd-3331-4c7e-9c95-e6880ae73603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path = \"./DeepSeek-R1\"  # Local directory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# üîÅ Inference function\n",
    "def chat(prompt: str, max_tokens: int = 512) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7830a3-b1e7-4160-af8e-7642cd7ae49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"Explain the concept of black holes in 10 words.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd05f86-6128-4367-b93a-86168575e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a34100-c3db-4ed5-9dbc-a1e177cac9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/voiceai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Model Loaded\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > Model's license - apache 2.0\n",
      " > Check https://choosealicense.com/licenses/apache-2.0/ for more info.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Model's license - apache 2.0\n",
      " > Check https://choosealicense.com/licenses/apache-2.0/ for more info.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/voiceai/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "TTS Model Loaded\n",
      "üéôÔ∏è Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM sysdefault\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM sysdefault\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.front\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround21\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround40\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround41\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround50\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround51\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.surround71\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.iec958\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.hdmi\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.modem\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.phoneline\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_id returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM dmix\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -9996] Invalid input device (no default output device)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m         wf.writeframes(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(frames))\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Call the function to record\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# üîπ Function to transcribe audio to text using the ASR model\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranscribe_audio\u001b[39m(audio_path=\u001b[33m\"\u001b[39m\u001b[33minput.wav\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrecord_audio\u001b[39m\u001b[34m(filename, duration, samplerate)\u001b[39m\n\u001b[32m     27\u001b[39m p = pyaudio.PyAudio()\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Set parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m stream = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mpyaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpaInt16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m                \u001b[49m\u001b[43mframes_per_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m frames = []\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(samplerate / \u001b[32m1024\u001b[39m * duration)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/voiceai/lib/python3.11/site-packages/pyaudio.py:750\u001b[39m, in \u001b[36mPyAudio.open\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    743\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[33;03m    Open a new stream. See constructor for\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03m    :py:func:`Stream.__init__` for parameter details.\u001b[39;00m\n\u001b[32m    746\u001b[39m \n\u001b[32m    747\u001b[39m \u001b[33;03m    :returns: A new :py:class:`Stream`\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     stream = \u001b[43mStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m     \u001b[38;5;28mself\u001b[39m._streams.add(stream)\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/voiceai/lib/python3.11/site-packages/pyaudio.py:441\u001b[39m, in \u001b[36mStream.__init__\u001b[39m\u001b[34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[39m\n\u001b[32m    438\u001b[39m     arguments[\u001b[33m'\u001b[39m\u001b[33mstream_callback\u001b[39m\u001b[33m'\u001b[39m] = stream_callback\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# calling pa.open returns a stream object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m \u001b[38;5;28mself\u001b[39m._stream = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m._input_latency = \u001b[38;5;28mself\u001b[39m._stream.inputLatency\n\u001b[32m    444\u001b[39m \u001b[38;5;28mself\u001b[39m._output_latency = \u001b[38;5;28mself\u001b[39m._stream.outputLatency\n",
      "\u001b[31mOSError\u001b[39m: [Errno -9996] Invalid input device (no default output device)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# import sounddevice as sd\n",
    "import scipy.io.wavfile\n",
    "import os\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from TTS.api import TTS\n",
    "\n",
    "# üîπ Initialize ASR model (Whisper)\n",
    "asr_model = WhisperModel(\"tiny.en\", device=\"cuda\")  # Change to \"cuda\" for GPU\n",
    "print(\"ASR Model Loaded\")\n",
    "\n",
    "# üîπ Initialize LLM model (Phi-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")  # Make sure to download model locally\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", device_map=\"cuda\")\n",
    "print(\"LLM Model Loaded\")\n",
    "\n",
    "# üîπ Initialize TTS model (Coqui TTS)\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False)\n",
    "print(\"TTS Model Loaded\")\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(filename=\"input.wav\", duration=5, samplerate=16000):\n",
    "    print(\"üéôÔ∏è Recording...\")\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Set parameters\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=samplerate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(0, int(samplerate / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(f\"‚úÖ Recording complete. Saving to {filename}\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Save the file\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(samplerate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "# Call the function to record\n",
    "record_audio(duration=5)\n",
    "\n",
    "\n",
    "# üîπ Function to transcribe audio to text using the ASR model\n",
    "def transcribe_audio(audio_path=\"input.wav\"):\n",
    "    segments, _ = asr_model.transcribe(audio_path)\n",
    "    transcript = \" \".join([seg.text for seg in segments])\n",
    "    print(f\"üëÇ Transcribed: {transcript}\")\n",
    "    return transcript\n",
    "\n",
    "# üîπ Function to generate a response using the LLM model\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Use \"cuda\" for GPU\n",
    "    outputs = llm_model.generate(inputs.input_ids, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"ü§ñ LLM Response: {response}\")\n",
    "    return response\n",
    "\n",
    "# üîπ Function to convert text to speech and save the output\n",
    "def speak_text(text, filename=\"output.wav\"):\n",
    "    tts.tts_to_file(text=text, file_path=filename)\n",
    "    os.system(f\"aplay {filename}\")  # On Linux/macOS, use pyaudio or sounddevice on Windows\n",
    "    print(f\"üîä Speaking: {text}\")\n",
    "\n",
    "# üîÅ Main loop to keep the system running\n",
    "def main():\n",
    "    while True:\n",
    "        # 1. Record audio\n",
    "        record_audio(duration=5)  # 5 seconds recording duration\n",
    "\n",
    "        # 2. Transcribe audio\n",
    "        transcript = transcribe_audio()\n",
    "\n",
    "        # 3. Generate response\n",
    "        response = generate_response(transcript)\n",
    "\n",
    "        # 4. Speak the response\n",
    "        \n",
    "        speak_text(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe4428b-ca4a-4964-a8a3-26b24d2af928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in /root/miniconda3/envs/voiceai/lib/python3.11/site-packages (0.2.11)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99777cfb-de31-4e05-adaa-3625be62a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow==9.5.0\n",
    "# !pip install TTS==0.15.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae3125-7497-4375-92f9-873f96f55877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
